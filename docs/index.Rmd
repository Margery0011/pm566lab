---
title: "lab07"
author: "yutian"
date: "10/6/2022"
output: html_document
---

```{r}
library(stringr)
library(rvest)
library(httr)
library(tidyverse)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1: How many sars-cov-2 papers?

```{r}

# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]")

# Turning it into text
counts <- as.character(counts)

```

```{r}
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
```

# Question 2: Academic publications on COVID19 and Hawaii


```{r}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db     = "pubmed", 
    term   = "covid19 hawaii", 
    retmax = 1000
    )
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
```
# Question 3: Get details about the articles


The Ids are wrapped around text in the following way: <Id>... id number ...</Id>. we can use a regular expression that extract that information. Fill out the following lines of code:


```{r get-ids, eval = TRUE}
# Turn the result into a character vector
ids <- as.character(ids)
# to print out text version of it use cat(ids) 
# Find all the ids  "[[1]]" :return string vector
ids <- stringr::str_extract_all(ids, "<Id>[0-9]+</Id>")[[1]]
# it can also work when you are dealing with 2 document
#ids <- stringr::str_extract_all(c(ids1,ids2), "<Id>[0-9]+</Id>")
# Remove all the leading and trailing <Id> </Id>. 
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
#alternative </?Id>
```

Grab publications with the pubmed ID list

```{r get-abstracts, eval = TRUE}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db      = "pubmed",
    id      = paste(ids, collapse = ","),
    retmax  = 1000,
    rettype = "abstract"
    )
)
publications

publications <- httr::content(publications)
publications_txt <- as.character(publications)
```

## Question 4: Distribution of universities, schools, and departments

Using the function `stringr::str_extract_all()` applied on `publications_txt`, capture all the terms of the form:

1.    University of ...
2.    ... Institute of ...

Write a regular expression that captures all such instances

```{r univ-institute-regex, eval = TRUE}
library(stringr)
institution <- str_extract_all(
  publications_txt,
  "University of\\s+[[:alpha:]]+|[[:alpha:]]+\\s+Institute of\\s+[[:alpha:]]+"
  ) 
#\\s+ means one or more space
institution <- unlist(institution)
table(institution)
```

Repeat the exercise and this time focus on schools and departments in the form of

1.    School of ...
2.    Department of ...

And tabulate the results

```{r school-department, eval = TRUE}
schools_and_deps <- str_extract_all(
  publications_txt,
  "School of\\s+[[:alpha:]]+|Department of\\s+[[:alpha:]]+"
  )
table(schools_and_deps)
```


## Question 5: Form a database

We want to build a dataset which includes the title and the abstract of the
paper. The title of all records is enclosed by the HTML tag `ArticleTitle`, and
the abstract by `Abstract`. 

Before applying the functions to extract text directly, it will help to process
the XML a bit. We will use the `xml2::xml_children()` function to keep one element
per id. This way, if a paper is missing the abstract, or something else, we will be able to properly match PUBMED IDS with their corresponding records.


```{r one-string-per-response, eval = TRUE}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
#take a look at it cat(pub_char_list[1])
```

Now, extract the abstract and article title for each one of the elements of
`pub_char_list`. You can either use `sapply()` as we just did, or simply
take advantage of vectorization of `stringr::str_extract`


```{r extracting-last-bit, eval = TRUE}
abstracts <- str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
#remove html text
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]]+>")
#remove new line and spaces
abstracts <- str_replace_all(abstracts, "\\s+"," ")
```

How many of these don't have an abstract?

```{r process-titles, eval = TRUE}
titles <- str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+"," ")
```

Finally, put everything together into a single `data.frame` and use
`knitr::kable` to print the results

Finally the dataset:

```{r}
database <- data.frame(
  PubMedId = ids,
  Title    = titles,
  Abstract = abstracts
)
knitr::kable(database[1:8,], caption = "Some papers about Covid19 and Hawaii")
```